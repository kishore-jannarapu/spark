version: '3'
services:
  spark-master:
    image: spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
  spark-worker-1:
    image: spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
  # spark-app:
  #   build:
  #     context: ./jobs/
  #   #image: spark-app
  #   container_name: spark-app
  #   environment: 
  #     - ENABLE_INIT_DAEMON=false
  postgres:
        image: postgres:9.6
        container_name: postgres
        restart: always
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"
  airflow:
      build:
          context: ./airflow
      image: airflow
      restart: always
      container_name: airflow
      depends_on:
          - postgres
      environment:
          - LOAD_EX=n
          - EXECUTOR=Local
      logging:
          options:
              max-size: 10m
              max-file: "3"
      volumes:
          - ./dags:/usr/local/airflow/dags
          - ./data:/data
          - ./plugins:/usr/local/airflow/plugins
      ports:
          - "7070:8080"
      command: webserver
      healthcheck:
          test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
          interval: 30s
          timeout: 30s
          retries: 3
